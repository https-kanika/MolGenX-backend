{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-01T04:33:23.402049Z",
     "iopub.status.busy": "2025-06-01T04:33:23.401723Z",
     "iopub.status.idle": "2025-06-01T04:33:23.772592Z",
     "shell.execute_reply": "2025-06-01T04:33:23.766435Z",
     "shell.execute_reply.started": "2025-06-01T04:33:23.402019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bindingdb-for-dta/Ki_bind.tsv\n",
      "/kaggle/input/bindingdb-for-dta/IC50_bind.tsv\n",
      "/kaggle/input/bindingdb-for-dta/Kd_bind.tsv\n",
      "/kaggle/input/bindingdb-for-dta/EC50_bind.tsv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:33:23.774144Z",
     "iopub.status.busy": "2025-06-01T04:33:23.773800Z",
     "iopub.status.idle": "2025-06-01T04:33:28.851448Z",
     "shell.execute_reply": "2025-06-01T04:33:28.850793Z",
     "shell.execute_reply.started": "2025-06-01T04:33:23.774121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>target_seq</th>\n",
       "      <th>origin_affinity</th>\n",
       "      <th>affinity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>P04183</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIA...</td>\n",
       "      <td>200</td>\n",
       "      <td>6.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P11413</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MAEQVALSRTQVCGILREELFQGDAFHQSDTHIFIIMGASGDLAKK...</td>\n",
       "      <td>1.54e+4</td>\n",
       "      <td>4.812479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>P23919</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MAARRGALIVLEGVDRAGKSTQSRKLVEALCAAGHRAELLRFPERS...</td>\n",
       "      <td>180000</td>\n",
       "      <td>3.744727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>P25099</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MPPYISAFQAAYIGIEVLIALVSVPGNVLVIWAVKVNQALRDATFC...</td>\n",
       "      <td>&gt;10000</td>\n",
       "      <td>4.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>P30543</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MGSSVYITVELAIAVLAILGNVLVCWAVWINSNLQNVTNFFVVSLA...</td>\n",
       "      <td>&gt;10000</td>\n",
       "      <td>4.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380360</th>\n",
       "      <td>9995</td>\n",
       "      <td>P11511</td>\n",
       "      <td>C[C@]12CCC3C(C[C@@H](O)C4=CCCC[C@]34CO)C1CCC2=O</td>\n",
       "      <td>MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...</td>\n",
       "      <td>110</td>\n",
       "      <td>6.958607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380361</th>\n",
       "      <td>9996</td>\n",
       "      <td>P11511</td>\n",
       "      <td>C[C@]12CCC3C(CCC4=CCCC[C@]34C)C1CCC2=O</td>\n",
       "      <td>MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>8.167491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380362</th>\n",
       "      <td>9997</td>\n",
       "      <td>P11511</td>\n",
       "      <td>C[C@]12CCC3C(CCC4=CCCC[C@]34CO)C1CCC2=O</td>\n",
       "      <td>MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>8.236572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380363</th>\n",
       "      <td>9998</td>\n",
       "      <td>P11511</td>\n",
       "      <td>C[C@]12CCC3C(CC=C4CCCC[C@]34C)C1CCC2=O</td>\n",
       "      <td>MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...</td>\n",
       "      <td>120</td>\n",
       "      <td>6.920819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380364</th>\n",
       "      <td>9999</td>\n",
       "      <td>P11511</td>\n",
       "      <td>C[C@]12CCC3C(CC=C4CCCC[C@]34CO)C1CCC2=O</td>\n",
       "      <td>MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380365 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        drug_id target_id                                           smiles  \\\n",
       "0             1    P04183  Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "1             1    P11413  Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "2             1    P23919  Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "3             1    P25099  Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "4             1    P30543  Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "...         ...       ...                                              ...   \n",
       "380360     9995    P11511  C[C@]12CCC3C(C[C@@H](O)C4=CCCC[C@]34CO)C1CCC2=O   \n",
       "380361     9996    P11511           C[C@]12CCC3C(CCC4=CCCC[C@]34C)C1CCC2=O   \n",
       "380362     9997    P11511          C[C@]12CCC3C(CCC4=CCCC[C@]34CO)C1CCC2=O   \n",
       "380363     9998    P11511           C[C@]12CCC3C(CC=C4CCCC[C@]34C)C1CCC2=O   \n",
       "380364     9999    P11511          C[C@]12CCC3C(CC=C4CCCC[C@]34CO)C1CCC2=O   \n",
       "\n",
       "                                               target_seq origin_affinity  \\\n",
       "0       MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIA...             200   \n",
       "1       MAEQVALSRTQVCGILREELFQGDAFHQSDTHIFIIMGASGDLAKK...         1.54e+4   \n",
       "2       MAARRGALIVLEGVDRAGKSTQSRKLVEALCAAGHRAELLRFPERS...          180000   \n",
       "3       MPPYISAFQAAYIGIEVLIALVSVPGNVLVIWAVKVNQALRDATFC...          >10000   \n",
       "4       MGSSVYITVELAIAVLAILGNVLVCWAVWINSNLQNVTNFFVVSLA...          >10000   \n",
       "...                                                   ...             ...   \n",
       "380360  MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...             110   \n",
       "380361  MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...             6.8   \n",
       "380362  MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...             5.8   \n",
       "380363  MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...             120   \n",
       "380364  MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...            1000   \n",
       "\n",
       "        affinity  \n",
       "0       6.698970  \n",
       "1       4.812479  \n",
       "2       3.744727  \n",
       "3       4.999957  \n",
       "4       4.999957  \n",
       "...          ...  \n",
       "380360  6.958607  \n",
       "380361  8.167491  \n",
       "380362  8.236572  \n",
       "380363  6.920819  \n",
       "380364  6.000000  \n",
       "\n",
       "[380365 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/bindingdb-for-dta/Ki_bind.tsv\", sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:33:28.852475Z",
     "iopub.status.busy": "2025-06-01T04:33:28.852201Z",
     "iopub.status.idle": "2025-06-01T04:33:44.122901Z",
     "shell.execute_reply": "2025-06-01T04:33:44.122265Z",
     "shell.execute_reply.started": "2025-06-01T04:33:28.852439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>target_seq</th>\n",
       "      <th>origin_affinity</th>\n",
       "      <th>affinity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>P04183</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIA...</td>\n",
       "      <td>16</td>\n",
       "      <td>7.795880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P06479</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MASYPGHQHASAFDQAARSRGHSNRRTALRPRRQQEATEVRPEQKM...</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>P11413</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MAEQVALSRTQVCGILREELFQGDAFHQSDTHIFIIMGASGDLAKK...</td>\n",
       "      <td>2.1e+4</td>\n",
       "      <td>4.677781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>P11511</td>\n",
       "      <td>COc1ccc2cc(oc2c1)C(O)(c1ccc(F)cc1)c1cccnc1</td>\n",
       "      <td>MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...</td>\n",
       "      <td>44</td>\n",
       "      <td>7.356547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>P11511</td>\n",
       "      <td>COc1ccc2cc(oc2c1)C(O)(c1ccc(Cl)cc1)c1cccnc1</td>\n",
       "      <td>MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...</td>\n",
       "      <td>49</td>\n",
       "      <td>7.309804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034575</th>\n",
       "      <td>99991</td>\n",
       "      <td>P30556</td>\n",
       "      <td>CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1nc(=O)o[n...</td>\n",
       "      <td>MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>8.744727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034576</th>\n",
       "      <td>99992</td>\n",
       "      <td>P30556</td>\n",
       "      <td>CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1nc(=O)o[n...</td>\n",
       "      <td>MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>8.886057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034577</th>\n",
       "      <td>99993</td>\n",
       "      <td>P30556</td>\n",
       "      <td>CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1nc(=O)o[n...</td>\n",
       "      <td>MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>8.853872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034578</th>\n",
       "      <td>99994</td>\n",
       "      <td>P30556</td>\n",
       "      <td>CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1nc(=O)o[n...</td>\n",
       "      <td>MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>8.721246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034579</th>\n",
       "      <td>99995</td>\n",
       "      <td>P30556</td>\n",
       "      <td>CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1noc(=O)[n...</td>\n",
       "      <td>MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8.795880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1034580 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         drug_id target_id                                             smiles  \\\n",
       "0              1    P04183    Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "1              1    P06479    Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "2              1    P11413    Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "3          10000    P11511         COc1ccc2cc(oc2c1)C(O)(c1ccc(F)cc1)c1cccnc1   \n",
       "4          10001    P11511        COc1ccc2cc(oc2c1)C(O)(c1ccc(Cl)cc1)c1cccnc1   \n",
       "...          ...       ...                                                ...   \n",
       "1034575    99991    P30556  CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1nc(=O)o[n...   \n",
       "1034576    99992    P30556  CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1nc(=O)o[n...   \n",
       "1034577    99993    P30556  CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1nc(=O)o[n...   \n",
       "1034578    99994    P30556  CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1nc(=O)o[n...   \n",
       "1034579    99995    P30556  CCc1cc2c(s1)n(Cc1ccc(cc1)-c1ccccc1-c1noc(=O)[n...   \n",
       "\n",
       "                                                target_seq origin_affinity  \\\n",
       "0        MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIA...              16   \n",
       "1        MASYPGHQHASAFDQAARSRGHSNRRTALRPRRQQEATEVRPEQKM...            1000   \n",
       "2        MAEQVALSRTQVCGILREELFQGDAFHQSDTHIFIIMGASGDLAKK...          2.1e+4   \n",
       "3        MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...              44   \n",
       "4        MVLEMLNPIHYNITSIVPEAMPAATMPVLLLTGLFLLVWNYEGTSS...              49   \n",
       "...                                                    ...             ...   \n",
       "1034575  MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...             1.8   \n",
       "1034576  MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...             1.3   \n",
       "1034577  MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...             1.4   \n",
       "1034578  MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...             1.9   \n",
       "1034579  MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGN...             1.6   \n",
       "\n",
       "         affinity  \n",
       "0        7.795880  \n",
       "1        6.000000  \n",
       "2        4.677781  \n",
       "3        7.356547  \n",
       "4        7.309804  \n",
       "...           ...  \n",
       "1034575  8.744727  \n",
       "1034576  8.886057  \n",
       "1034577  8.853872  \n",
       "1034578  8.721246  \n",
       "1034579  8.795880  \n",
       "\n",
       "[1034580 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"/kaggle/input/bindingdb-for-dta/IC50_bind.tsv\", sep='\\t')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:33:44.124825Z",
     "iopub.status.busy": "2025-06-01T04:33:44.124609Z",
     "iopub.status.idle": "2025-06-01T04:33:44.131488Z",
     "shell.execute_reply": "2025-06-01T04:33:44.130782Z",
     "shell.execute_reply.started": "2025-06-01T04:33:44.124806Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['drug_id', 'target_id', 'smiles', 'target_seq', 'origin_affinity',\n",
       "       'affinity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:33:44.132559Z",
     "iopub.status.busy": "2025-06-01T04:33:44.132319Z",
     "iopub.status.idle": "2025-06-01T04:33:57.406570Z",
     "shell.execute_reply": "2025-06-01T04:33:57.406024Z",
     "shell.execute_reply.started": "2025-06-01T04:33:44.132514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>target_seq</th>\n",
       "      <th>affinity</th>\n",
       "      <th>AffinityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIA...</td>\n",
       "      <td>6.698970</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MAEQVALSRTQVCGILREELFQGDAFHQSDTHIFIIMGASGDLAKK...</td>\n",
       "      <td>4.812479</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MAARRGALIVLEGVDRAGKSTQSRKLVEALCAAGHRAELLRFPERS...</td>\n",
       "      <td>3.744727</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MPPYISAFQAAYIGIEVLIALVSVPGNVLVIWAVKVNQALRDATFC...</td>\n",
       "      <td>4.999957</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MGSSVYITVELAIAVLAILGNVLVCWAVWINSNLQNVTNFFVVSLA...</td>\n",
       "      <td>4.999957</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635682</th>\n",
       "      <td>CN1CCN(CC1)c1cccc(Nc2nc3c(cccn3n2)-c2ccc(cc2)S...</td>\n",
       "      <td>MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...</td>\n",
       "      <td>6.694649</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635683</th>\n",
       "      <td>Cc1cc(NCCCCO)n2c(nc3ccccc23)c1C#N</td>\n",
       "      <td>MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...</td>\n",
       "      <td>4.157953</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635684</th>\n",
       "      <td>CN1C(N(C)c2ccccc2C1=O)c1ccc2OCOc2c1</td>\n",
       "      <td>MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...</td>\n",
       "      <td>4.158053</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635685</th>\n",
       "      <td>COc1ccc(C2C3C(=O)CC(C)(C)CC3=NC(C)=C2C(=O)N2CC...</td>\n",
       "      <td>MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...</td>\n",
       "      <td>4.280470</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635686</th>\n",
       "      <td>COc1ccc(cc1)-c1cc(-c2ccccc2)n2ncnc2n1</td>\n",
       "      <td>MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...</td>\n",
       "      <td>4.157928</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1635687 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    smiles  \\\n",
       "0          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "1          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "2          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "3          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "4          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "...                                                    ...   \n",
       "1635682  CN1CCN(CC1)c1cccc(Nc2nc3c(cccn3n2)-c2ccc(cc2)S...   \n",
       "1635683                  Cc1cc(NCCCCO)n2c(nc3ccccc23)c1C#N   \n",
       "1635684                CN1C(N(C)c2ccccc2C1=O)c1ccc2OCOc2c1   \n",
       "1635685  COc1ccc(C2C3C(=O)CC(C)(C)CC3=NC(C)=C2C(=O)N2CC...   \n",
       "1635686              COc1ccc(cc1)-c1cc(-c2ccccc2)n2ncnc2n1   \n",
       "\n",
       "                                                target_seq  affinity  \\\n",
       "0        MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIA...  6.698970   \n",
       "1        MAEQVALSRTQVCGILREELFQGDAFHQSDTHIFIIMGASGDLAKK...  4.812479   \n",
       "2        MAARRGALIVLEGVDRAGKSTQSRKLVEALCAAGHRAELLRFPERS...  3.744727   \n",
       "3        MPPYISAFQAAYIGIEVLIALVSVPGNVLVIWAVKVNQALRDATFC...  4.999957   \n",
       "4        MGSSVYITVELAIAVLAILGNVLVCWAVWINSNLQNVTNFFVVSLA...  4.999957   \n",
       "...                                                    ...       ...   \n",
       "1635682  MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...  6.694649   \n",
       "1635683  MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...  4.157953   \n",
       "1635684  MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...  4.158053   \n",
       "1635685  MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...  4.280470   \n",
       "1635686  MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...  4.157928   \n",
       "\n",
       "        AffinityType  \n",
       "0                 Ki  \n",
       "1                 Ki  \n",
       "2                 Ki  \n",
       "3                 Ki  \n",
       "4                 Ki  \n",
       "...              ...  \n",
       "1635682         EC50  \n",
       "1635683         EC50  \n",
       "1635684         EC50  \n",
       "1635685         EC50  \n",
       "1635686         EC50  \n",
       "\n",
       "[1635687 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_file(path, affinity_type):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    df = df[['smiles', 'target_seq', 'affinity']].dropna()\n",
    "    df['AffinityType'] = affinity_type\n",
    "    return df\n",
    "\n",
    "df_ki = load_file('/kaggle/input/bindingdb-for-dta/Ki_bind.tsv', 'Ki')\n",
    "df_ic50 = load_file('/kaggle/input/bindingdb-for-dta/IC50_bind.tsv', 'IC50')\n",
    "df_kd = load_file('/kaggle/input/bindingdb-for-dta/Kd_bind.tsv', 'Kd')\n",
    "df_ec50 = load_file('/kaggle/input/bindingdb-for-dta/EC50_bind.tsv', 'EC50')\n",
    "\n",
    "# Merge them into one\n",
    "df = pd.concat([df_ki, df_ic50, df_kd, df_ec50], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:33:57.407461Z",
     "iopub.status.busy": "2025-06-01T04:33:57.407254Z",
     "iopub.status.idle": "2025-06-01T04:33:58.027319Z",
     "shell.execute_reply": "2025-06-01T04:33:58.026582Z",
     "shell.execute_reply.started": "2025-06-01T04:33:57.407437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/2390296033.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['affinity'] = np.log(df['affinity'])\n",
      "/tmp/ipykernel_35/2390296033.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['affinity'] = scaler.fit_transform(df[['affinity']])\n",
      "/tmp/ipykernel_35/2390296033.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={'smiles': 'Drug', 'target_seq': 'Target', 'affinity': 'Affinity'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug</th>\n",
       "      <th>Target</th>\n",
       "      <th>Affinity</th>\n",
       "      <th>AffinityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIA...</td>\n",
       "      <td>0.884738</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MAEQVALSRTQVCGILREELFQGDAFHQSDTHIFIIMGASGDLAKK...</td>\n",
       "      <td>0.840486</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MAARRGALIVLEGVDRAGKSTQSRKLVEALCAAGHRAELLRFPERS...</td>\n",
       "      <td>0.806921</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MPPYISAFQAAYIGIEVLIALVSVPGNVLVIWAVKVNQALRDATFC...</td>\n",
       "      <td>0.845599</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O</td>\n",
       "      <td>MGSSVYITVELAIAVLAILGNVLVCWAVWINSNLQNVTNFFVVSLA...</td>\n",
       "      <td>0.845599</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635682</th>\n",
       "      <td>CN1CCN(CC1)c1cccc(Nc2nc3c(cccn3n2)-c2ccc(cc2)S...</td>\n",
       "      <td>MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...</td>\n",
       "      <td>0.884652</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635683</th>\n",
       "      <td>Cc1cc(NCCCCO)n2c(nc3ccccc23)c1C#N</td>\n",
       "      <td>MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...</td>\n",
       "      <td>0.820926</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635684</th>\n",
       "      <td>CN1C(N(C)c2ccccc2C1=O)c1ccc2OCOc2c1</td>\n",
       "      <td>MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...</td>\n",
       "      <td>0.820929</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635685</th>\n",
       "      <td>COc1ccc(C2C3C(=O)CC(C)(C)CC3=NC(C)=C2C(=O)N2CC...</td>\n",
       "      <td>MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...</td>\n",
       "      <td>0.824811</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635686</th>\n",
       "      <td>COc1ccc(cc1)-c1cc(-c2ccccc2)n2ncnc2n1</td>\n",
       "      <td>MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...</td>\n",
       "      <td>0.820925</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1634632 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Drug  \\\n",
       "0          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "1          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "2          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "3          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "4          Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "...                                                    ...   \n",
       "1635682  CN1CCN(CC1)c1cccc(Nc2nc3c(cccn3n2)-c2ccc(cc2)S...   \n",
       "1635683                  Cc1cc(NCCCCO)n2c(nc3ccccc23)c1C#N   \n",
       "1635684                CN1C(N(C)c2ccccc2C1=O)c1ccc2OCOc2c1   \n",
       "1635685  COc1ccc(C2C3C(=O)CC(C)(C)CC3=NC(C)=C2C(=O)N2CC...   \n",
       "1635686              COc1ccc(cc1)-c1cc(-c2ccccc2)n2ncnc2n1   \n",
       "\n",
       "                                                    Target  Affinity  \\\n",
       "0        MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIA...  0.884738   \n",
       "1        MAEQVALSRTQVCGILREELFQGDAFHQSDTHIFIIMGASGDLAKK...  0.840486   \n",
       "2        MAARRGALIVLEGVDRAGKSTQSRKLVEALCAAGHRAELLRFPERS...  0.806921   \n",
       "3        MPPYISAFQAAYIGIEVLIALVSVPGNVLVIWAVKVNQALRDATFC...  0.845599   \n",
       "4        MGSSVYITVELAIAVLAILGNVLVCWAVWINSNLQNVTNFFVVSLA...  0.845599   \n",
       "...                                                    ...       ...   \n",
       "1635682  MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...  0.884652   \n",
       "1635683  MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...  0.820926   \n",
       "1635684  MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...  0.820929   \n",
       "1635685  MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...  0.824811   \n",
       "1635686  MARSLLLPLQILLLSLALETAGEEAQGDKIIDGAPCARGSHPWQVA...  0.820925   \n",
       "\n",
       "        AffinityType  \n",
       "0                 Ki  \n",
       "1                 Ki  \n",
       "2                 Ki  \n",
       "3                 Ki  \n",
       "4                 Ki  \n",
       "...              ...  \n",
       "1635682         EC50  \n",
       "1635683         EC50  \n",
       "1635684         EC50  \n",
       "1635685         EC50  \n",
       "1635686         EC50  \n",
       "\n",
       "[1634632 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = df[df['affinity'] > 0]  # avoid log(0)\n",
    "df['affinity'] = np.log(df['affinity'])\n",
    "scaler = MinMaxScaler()\n",
    "df['affinity'] = scaler.fit_transform(df[['affinity']])\n",
    "\n",
    "# Rename for consistency with model code\n",
    "df.rename(columns={'smiles': 'Drug', 'target_seq': 'Target', 'affinity': 'Affinity'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:33:58.028746Z",
     "iopub.status.busy": "2025-06-01T04:33:58.028323Z",
     "iopub.status.idle": "2025-06-01T04:33:59.380998Z",
     "shell.execute_reply": "2025-06-01T04:33:59.380347Z",
     "shell.execute_reply.started": "2025-06-01T04:33:58.028721Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3885303606.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['smiles_len'] = df['Drug'].apply(len)\n",
      "/tmp/ipykernel_35/3885303606.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target_len'] = df['Target'].apply(len)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['smiles_len'] = df['Drug'].apply(len)\n",
    "df['target_len'] = df['Target'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['smiles_len'], bins=50, color='teal', alpha=0.7)\n",
    "plt.title(\"SMILES Length Distribution\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['target_len'], bins=50, color='orange', alpha=0.7)\n",
    "plt.title(\"Target Sequence Length Distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:33:59.381962Z",
     "iopub.status.busy": "2025-06-01T04:33:59.381704Z",
     "iopub.status.idle": "2025-06-01T04:34:01.390383Z",
     "shell.execute_reply": "2025-06-01T04:34:01.389546Z",
     "shell.execute_reply.started": "2025-06-01T04:33:59.381935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/631744863.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Drug'] = df['Drug'].apply(lambda x: pad_or_truncate(x, max_len_drug))\n",
      "/tmp/ipykernel_35/631744863.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Target'] = df['Target'].apply(lambda x: pad_or_truncate(x, max_len_target))\n"
     ]
    }
   ],
   "source": [
    "max_len_drug = 100\n",
    "max_len_target = 512\n",
    "\n",
    "def pad_or_truncate(seq, max_len, pad_char=' '):\n",
    "    seq = str(seq)\n",
    "    if len(seq) > max_len:\n",
    "        return seq[:max_len]\n",
    "    else:\n",
    "        return seq + pad_char * (max_len - len(seq))\n",
    "\n",
    "df['Drug'] = df['Drug'].apply(lambda x: pad_or_truncate(x, max_len_drug))\n",
    "df['Target'] = df['Target'].apply(lambda x: pad_or_truncate(x, max_len_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:35:17.116205Z",
     "iopub.status.busy": "2025-06-01T04:35:17.115460Z",
     "iopub.status.idle": "2025-06-01T04:35:17.119752Z",
     "shell.execute_reply": "2025-06-01T04:35:17.119048Z",
     "shell.execute_reply.started": "2025-06-01T04:35:17.116172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df=df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:34:41.071648Z",
     "iopub.status.busy": "2025-06-01T04:34:41.071012Z",
     "iopub.status.idle": "2025-06-01T04:34:47.064180Z",
     "shell.execute_reply": "2025-06-01T04:34:47.063367Z",
     "shell.execute_reply.started": "2025-06-01T04:34:41.071625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2025.3.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit) (2024.2.0)\n",
      "Downloading rdkit-2025.3.2-cp311-cp311-manylinux_2_28_x86_64.whl (35.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rdkit\n",
      "Successfully installed rdkit-2025.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:39:23.191803Z",
     "iopub.status.busy": "2025-06-01T04:39:23.190982Z",
     "iopub.status.idle": "2025-06-01T04:41:08.429686Z",
     "shell.execute_reply": "2025-06-01T04:41:08.428931Z",
     "shell.execute_reply.started": "2025-06-01T04:39:23.191777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing existing DataFrame...\n",
      "DataFrame has 100 samples\n",
      "SMILES vocabulary size: 23\n",
      "Target vocabulary size: 22\n",
      "Training data: 80 samples\n",
      "Validation data: 20 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ target_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ target_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ latent_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ bidirectional_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ affinity_input            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ concatenate_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ latent_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ bidirectional_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                           │                        │                │ affinity_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7680</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,973,760</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ bidirectional_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ time_distributed_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911</span> │ bidirectional_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ target_input (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │          \u001b[38;5;34m1,408\u001b[0m │ target_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ latent_input (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ bidirectional_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m66,048\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ affinity_input            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ concatenate_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ latent_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ bidirectional_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                           │                        │                │ affinity_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m24,832\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m33,024\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7680\u001b[0m)           │      \u001b[38;5;34m1,973,760\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ bidirectional_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m197,632\u001b[0m │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ time_distributed_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │          \u001b[38;5;34m5,911\u001b[0m │ bidirectional_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,302,615</span> (8.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,302,615\u001b[0m (8.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,302,615</span> (8.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,302,615\u001b[0m (8.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVAE model built and compiled.\n",
      "Training CVAE for 50 epochs with batch size 32...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748752784.029114     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 3.0935 - val_loss: 2.7727\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671ms/step - loss: 2.6290 - val_loss: 1.9421\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 661ms/step - loss: 1.8315 - val_loss: 1.6823\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 658ms/step - loss: 1.5542 - val_loss: 1.5191\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 662ms/step - loss: 1.4264 - val_loss: 1.4402\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 653ms/step - loss: 1.3870 - val_loss: 1.3973\n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 660ms/step - loss: 1.3238 - val_loss: 1.3846\n",
      "Epoch 8/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 673ms/step - loss: 1.3090 - val_loss: 1.3506\n",
      "Epoch 9/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 651ms/step - loss: 1.2688 - val_loss: 1.3166\n",
      "Epoch 10/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 657ms/step - loss: 1.2242 - val_loss: 1.2448\n",
      "Epoch 11/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 656ms/step - loss: 1.1461 - val_loss: 1.1462\n",
      "Epoch 12/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 663ms/step - loss: 1.0916 - val_loss: 1.0986\n",
      "Epoch 13/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 661ms/step - loss: 1.0417 - val_loss: 1.0982\n",
      "Epoch 14/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step - loss: 1.0297 - val_loss: 1.1051\n",
      "Epoch 15/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 656ms/step - loss: 0.9472 - val_loss: 1.0834\n",
      "Epoch 16/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570ms/step - loss: 0.9966 - val_loss: 1.1061\n",
      "Epoch 17/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 703ms/step - loss: 0.9399 - val_loss: 1.0479\n",
      "Epoch 18/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 669ms/step - loss: 0.9460 - val_loss: 1.0390\n",
      "Epoch 19/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671ms/step - loss: 0.9390 - val_loss: 1.0266\n",
      "Epoch 20/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 647ms/step - loss: 0.9328 - val_loss: 1.0042\n",
      "Epoch 21/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 656ms/step - loss: 0.9065 - val_loss: 0.9914\n",
      "Epoch 22/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562ms/step - loss: 0.8943 - val_loss: 0.9996\n",
      "Epoch 23/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 663ms/step - loss: 0.8705 - val_loss: 0.9772\n",
      "Epoch 24/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 666ms/step - loss: 0.8548 - val_loss: 0.9715\n",
      "Epoch 25/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571ms/step - loss: 0.8541 - val_loss: 0.9720\n",
      "Epoch 26/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 660ms/step - loss: 0.8301 - val_loss: 0.9572\n",
      "Epoch 27/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 672ms/step - loss: 0.8491 - val_loss: 0.9520\n",
      "Epoch 28/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568ms/step - loss: 0.8173 - val_loss: 0.9531\n",
      "Epoch 29/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563ms/step - loss: 0.8129 - val_loss: 0.9525\n",
      "Epoch 30/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 661ms/step - loss: 0.7704 - val_loss: 0.9389\n",
      "Epoch 31/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568ms/step - loss: 0.7949 - val_loss: 0.9630\n",
      "Epoch 32/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step - loss: 0.7467 - val_loss: 0.9432\n",
      "Epoch 33/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563ms/step - loss: 0.7521 - val_loss: 0.9453\n",
      "Epoch 34/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 675ms/step - loss: 0.7580 - val_loss: 0.9280\n",
      "Epoch 35/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 656ms/step - loss: 0.7436 - val_loss: 0.9179\n",
      "Epoch 36/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576ms/step - loss: 0.7090 - val_loss: 0.9224\n",
      "Epoch 37/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 654ms/step - loss: 0.7240 - val_loss: 0.9140\n",
      "Epoch 38/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 731ms/step - loss: 0.7050 - val_loss: 0.9032\n",
      "Epoch 39/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 652ms/step - loss: 0.7256 - val_loss: 0.8999\n",
      "Epoch 40/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step - loss: 0.6899 - val_loss: 0.9003\n",
      "Epoch 41/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571ms/step - loss: 0.6902 - val_loss: 0.9061\n",
      "Epoch 42/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570ms/step - loss: 0.6742 - val_loss: 0.9182\n",
      "Epoch 43/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step - loss: 0.6709 - val_loss: 0.9266\n",
      "Epoch 44/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 655ms/step - loss: 0.7060 - val_loss: 0.8976\n",
      "Epoch 45/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 660ms/step - loss: 0.6866 - val_loss: 0.8936\n",
      "Epoch 46/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570ms/step - loss: 0.6708 - val_loss: 0.9133\n",
      "Epoch 47/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 658ms/step - loss: 0.6623 - val_loss: 0.8902\n",
      "Epoch 48/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 660ms/step - loss: 0.6417 - val_loss: 0.8834\n",
      "Epoch 49/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 654ms/step - loss: 0.6202 - val_loss: 0.8773\n",
      "Epoch 50/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 663ms/step - loss: 0.6067 - val_loss: 0.8687\n",
      "Training completed. Models saved.\n",
      "Training history plotted and saved.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Generated 0 valid molecules out of 20 attempts.\n",
      "Generated molecules:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@c1ccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n",
      "[04:41:08] SMILES Parse Error: syntax error while parsing: c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)\n",
      "[04:41:08] SMILES Parse Error: check for mistakes around position 5:\n",
      "[04:41:08] c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cc\n",
      "[04:41:08] ~~~~^\n",
      "[04:41:08] SMILES Parse Error: Failed parsing SMILES 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)' for input: 'c[c@)c(coc2ccc(cl)cn2ccn(cccocc]1c(ccc(cccccc)cc=)@ccccn(ccccccccc(ccc)c#)'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Add these imports and checks at the beginning of your notebook, \n",
    "# before creating your model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import time  # For GPU benchmark\n",
    "# Add at the top of your notebook\n",
    "# Add after your imports\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Memory monitoring function\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"Monitor GPU memory usage during training\"\"\"\n",
    "    try:\n",
    "        # Get GPU memory stats\n",
    "        gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "        if not gpu_devices:\n",
    "            print(\"No GPU detected\")\n",
    "            return None\n",
    "        \n",
    "        # For TF 2.x\n",
    "        memory_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "        current_mb = memory_info['current'] / (1024 * 1024)\n",
    "        peak_mb = memory_info['peak'] / (1024 * 1024)\n",
    "        print(f\"GPU Memory: Current={current_mb:.2f}MB, Peak={peak_mb:.2f}MB\")\n",
    "        return current_mb\n",
    "    except Exception as e:\n",
    "        print(f\"Could not monitor GPU memory: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initialize callbacks list before using it\n",
    "callbacks = []  # Define this BEFORE trying to append to it\n",
    "\n",
    "# Now you can append to it\n",
    "callbacks.append(tf.keras.callbacks.BackupAndRestore(\n",
    "    backup_dir='./backup_checkpoints'\n",
    "))\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 1. Set environment variables for GPU memory growth\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce logging noise\n",
    "\n",
    "# 2. Better GPU detection and configuration\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU detection test:\")\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"✓ Found {len(gpu_devices)} GPU(s):\")\n",
    "    for i, device in enumerate(gpu_devices):\n",
    "        # Configure memory growth\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "            details = tf.config.experimental.get_device_details(device)\n",
    "            print(f\"  [{i}] {details.get('device_name', device.name)} ({device.device_type})\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"  Error configuring GPU {device}: {e}\")\n",
    "            \n",
    "    # Use mixed precision for Ampere architecture GPUs like the L4\n",
    "    try:\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(\"✓ Enabled mixed precision training (float16)\")\n",
    "    except:\n",
    "        print(\"✗ Could not enable mixed precision\")\n",
    "else:\n",
    "    print(\"✗ No GPUs found. Checking CUDA installation...\")\n",
    "    \n",
    "    # Check CUDA installation\n",
    "    if not tf.test.is_built_with_cuda():\n",
    "        print(\"  ✗ TensorFlow not built with CUDA support\")\n",
    "        print(\"  → Install tensorflow-gpu or tensorflow with CUDA support\")\n",
    "    \n",
    "    # Try to manually detect NVIDIA GPUs via system commands\n",
    "    try:\n",
    "        import subprocess\n",
    "        gpu_info = subprocess.check_output('nvidia-smi', shell=True).decode()\n",
    "        if 'NVIDIA' in gpu_info:\n",
    "            print(\"  ! NVIDIA GPU found via nvidia-smi but not recognized by TensorFlow\")\n",
    "            print(\"  → Check TensorFlow/CUDA compatibility\")\n",
    "            print(gpu_info.split('\\n')[0])\n",
    "    except:\n",
    "        print(\"  × Could not run nvidia-smi\")\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Reparameterization trick with improved numerical stability.\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        \n",
    "        # Clip z_log_var to prevent extreme values\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -20.0, 2.0)\n",
    "        \n",
    "        # Create epsilon with same dtype as inputs\n",
    "        epsilon = tf.random.normal(shape=(batch, dim), dtype=z_mean.dtype)\n",
    "        \n",
    "        # Return with clipped values\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "class KLDivergenceLayer(layers.Layer):\n",
    "    \"\"\"Layer that computes KL divergence loss with added stability.\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        # Clip values for stability\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -20.0, 2.0)\n",
    "        z_mean = tf.clip_by_value(z_mean, -20.0, 20.0)\n",
    "        \n",
    "        # Use reduce_mean instead of keras.backend for better stability\n",
    "        kl_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),\n",
    "            axis=-1\n",
    "        )\n",
    "        # Add as a loss with gradient clipping\n",
    "        self.add_loss(tf.reduce_mean(kl_loss))\n",
    "        return z_mean\n",
    "\n",
    "# 2. Define custom VAE loss layer\n",
    "class VAELoss(layers.Layer):\n",
    "    \"\"\"Custom layer to calculate VAE loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, smiles_max_length, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.smiles_max_length = smiles_max_length\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        y_true, y_pred, z_mean, z_log_var = inputs\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred) \n",
    "        reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=-1)\n",
    "        reconstruction_loss *= self.smiles_max_length\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), \n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        # Total loss\n",
    "        return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "class DrugDiscoveryCVAE:\n",
    "    def __init__(self, smiles_max_length=120, target_max_length=1000, latent_dim=64):\n",
    "        \"\"\"\n",
    "        Initialize the Conditional VAE model for drug discovery.\n",
    "        \n",
    "        Args:\n",
    "            smiles_max_length: Maximum length of SMILES strings\n",
    "            target_max_length: Maximum length of target protein sequences\n",
    "            latent_dim: Dimension of the latent space\n",
    "        \"\"\"\n",
    "        self.smiles_max_length = smiles_max_length\n",
    "        self.target_max_length = target_max_length\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Placeholders for tokenizers\n",
    "        self.smiles_tokenizer = None\n",
    "        self.smiles_vocab_size = None\n",
    "        self.target_tokenizer = None\n",
    "        self.target_vocab_size = None\n",
    "        \n",
    "        # Placeholders for models\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.cvae = None\n",
    "        \n",
    "        # Placeholder for affinity scaler\n",
    "        self.affinity_scaler = StandardScaler()\n",
    "        \n",
    "        # For visualization and tracking\n",
    "        self.history = None\n",
    "        self.models_dir = \"models\"\n",
    "        os.makedirs(self.models_dir, exist_ok=True)\n",
    "        \n",
    "\n",
    "    def preprocess_existing_dataframe(self, df):\n",
    "        \"\"\"\n",
    "        Preprocess an existing pandas DataFrame containing drug-target-affinity data.\n",
    "        \n",
    "        Args:\n",
    "            df: Pandas DataFrame with columns: 'Drug' (SMILES), 'Target' (protein sequence), 'Affinity'\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed data ready for training\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing existing DataFrame...\")\n",
    "        print(f\"DataFrame has {len(df)} samples\")\n",
    "        \n",
    "        # Create tokenizers for SMILES and target sequences\n",
    "        self.smiles_tokenizer = Tokenizer(char_level=True)\n",
    "        self.smiles_tokenizer.fit_on_texts(df['Drug'].values)\n",
    "        self.smiles_vocab_size = len(self.smiles_tokenizer.word_index) + 1\n",
    "        print(f\"SMILES vocabulary size: {self.smiles_vocab_size}\")\n",
    "        \n",
    "        self.target_tokenizer = Tokenizer(char_level=True)\n",
    "        self.target_tokenizer.fit_on_texts(df['Target'].values)\n",
    "        self.target_vocab_size = len(self.target_tokenizer.word_index) + 1\n",
    "        print(f\"Target vocabulary size: {self.target_vocab_size}\")\n",
    "        \n",
    "        # Convert sequences to integer indices\n",
    "        smiles_seqs = self.smiles_tokenizer.texts_to_sequences(df['Drug'].values)\n",
    "        target_seqs = self.target_tokenizer.texts_to_sequences(df['Target'].values)\n",
    "        \n",
    "        # Pad sequences to fixed length\n",
    "        smiles_data = pad_sequences(smiles_seqs, maxlen=self.smiles_max_length, padding='post')\n",
    "        target_data = pad_sequences(target_seqs, maxlen=self.target_max_length, padding='post')\n",
    "        \n",
    "        # One-hot encode SMILES for output layer\n",
    "        smiles_one_hot = tf.keras.utils.to_categorical(smiles_data, num_classes=self.smiles_vocab_size)\n",
    "        \n",
    "        # Scale affinities\n",
    "        affinities = df['Affinity'].values.reshape(-1, 1)\n",
    "        self.affinity_scaler.fit(affinities)\n",
    "        scaled_affinities = self.affinity_scaler.transform(affinities)\n",
    "        \n",
    "        # Split data into training and validation sets\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(df)), test_size=0.2, random_state=42)\n",
    "        \n",
    "        train_data = {\n",
    "            'smiles_data': smiles_data[train_indices],\n",
    "            'smiles_one_hot': smiles_one_hot[train_indices],\n",
    "            'target_data': target_data[train_indices],\n",
    "            'affinities': scaled_affinities[train_indices]\n",
    "        }\n",
    "        \n",
    "        val_data = {\n",
    "            'smiles_data': smiles_data[val_indices],\n",
    "            'smiles_one_hot': smiles_one_hot[val_indices],\n",
    "            'target_data': target_data[val_indices],\n",
    "            'affinities': scaled_affinities[val_indices]\n",
    "        }\n",
    "        \n",
    "        print(f\"Training data: {len(train_data['smiles_data'])} samples\")\n",
    "        print(f\"Validation data: {len(val_data['smiles_data'])} samples\")\n",
    "        \n",
    "        return train_data, val_data\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \"\"\"Build an ultra-simplified encoder for large datasets\"\"\"\n",
    "        # SMILES input\n",
    "        smiles_input = keras.Input(shape=(self.smiles_max_length,), name='smiles_input')\n",
    "        \n",
    "        # Target protein input\n",
    "        target_input = keras.Input(shape=(self.target_max_length,), name='target_input')\n",
    "        target_embedding = layers.Embedding(\n",
    "            input_dim=self.target_vocab_size,\n",
    "            output_dim=8,  # Reduced from 16\n",
    "            input_length=self.target_max_length\n",
    "        )(target_input)\n",
    "        # Replace LSTM with GlobalAveragePooling - much less GPU memory\n",
    "        target_features = layers.GlobalAveragePooling1D()(target_embedding)\n",
    "        \n",
    "        # Affinity input\n",
    "        affinity_input = keras.Input(shape=(1,), name='affinity_input')\n",
    "        \n",
    "        # Encode SMILES with minimal embedding\n",
    "        smiles_embedding = layers.Embedding(\n",
    "            input_dim=self.smiles_vocab_size,\n",
    "            output_dim=8,  # Reduced from 32\n",
    "            input_length=self.smiles_max_length\n",
    "        )(smiles_input)\n",
    "        \n",
    "        # Use GlobalAveragePooling instead of Conv1D - much faster and less memory\n",
    "        smiles_features = layers.Conv1D(filters=16, kernel_size=3, padding='same', activation='relu')(smiles_embedding)\n",
    "        smiles_features = layers.GlobalMaxPooling1D()(smiles_features)\n",
    "        # Concatenate features\n",
    "        x = layers.concatenate([smiles_features, target_features, affinity_input])\n",
    "        \n",
    "        # Very minimal dense layer\n",
    "        x = layers.Dense(16, activation='relu')(x)\n",
    "        \n",
    "        # Smaller latent space\n",
    "        self.latent_dim = 8  # Reduced from 16\n",
    "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(x)\n",
    "        \n",
    "        # Use our custom Sampling layer\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        \n",
    "        # Create encoder model\n",
    "        encoder = keras.Model(\n",
    "            inputs=[smiles_input, target_input, affinity_input],\n",
    "            outputs=[z_mean, z_log_var, z],\n",
    "            name='encoder'\n",
    "        )\n",
    "        \n",
    "        return encoder\n",
    "         \n",
    "        \n",
    "\n",
    "    def build_decoder(self):\n",
    "        \"\"\"Build an ultra-simplified decoder for large datasets\"\"\"\n",
    "        # Latent space input\n",
    "        latent_input = keras.Input(shape=(self.latent_dim,), name='latent_input')\n",
    "        \n",
    "        # Target protein input\n",
    "        target_input = keras.Input(shape=(self.target_max_length,), name='target_input')\n",
    "        target_embedding = layers.Embedding(\n",
    "            input_dim=self.target_vocab_size,\n",
    "            output_dim=8,  # Reduced from 16\n",
    "            input_length=self.target_max_length\n",
    "        )(target_input)\n",
    "        # Replace LSTM with GlobalAveragePooling\n",
    "        target_features = layers.GlobalAveragePooling1D()(target_embedding)\n",
    "        \n",
    "        # Affinity input\n",
    "        affinity_input = keras.Input(shape=(1,), name='affinity_input')\n",
    "        \n",
    "        # Concatenate inputs\n",
    "        x = layers.concatenate([latent_input, target_features, affinity_input])\n",
    "        \n",
    "        # Minimal dense processing\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        \n",
    "        # Reshape for sequence generation - even smaller dimensions\n",
    "        x = layers.Dense(self.smiles_max_length * 8, activation='relu')(x)  # Reduced from 16\n",
    "        x = layers.Reshape((self.smiles_max_length, 8))(x)\n",
    "        \n",
    "        # Simple output projection - no Conv1D\n",
    "        smiles_output = layers.TimeDistributed(\n",
    "            layers.Dense(self.smiles_vocab_size, activation='softmax')\n",
    "        )(x)\n",
    "        \n",
    "        # Create decoder model\n",
    "        decoder = keras.Model(\n",
    "            inputs=[latent_input, target_input, affinity_input],\n",
    "            outputs=smiles_output,\n",
    "            name='decoder'\n",
    "        )\n",
    "        \n",
    "        return decoder\n",
    "        \n",
    "        \n",
    "    def smiles_validity_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Custom loss function that penalizes sequences likely to be invalid SMILES.\n",
    "        \n",
    "        Args:\n",
    "            y_true: One-hot encoded ground truth SMILES\n",
    "            y_pred: Model predictions (probabilities)\n",
    "        \n",
    "        Returns:\n",
    "            Additional loss to penalize likely invalid sequences\n",
    "        \"\"\"\n",
    "        # Get most likely character at each position\n",
    "        char_indices = tf.argmax(y_pred, axis=-1)\n",
    "    \n",
    "        # Track grammar violations\n",
    "        \n",
    "        # 1. Penalty for unbalanced parentheses \n",
    "        # Get one-hot indices for open and close parentheses\n",
    "        open_paren_id = self.smiles_tokenizer.word_index.get('(', 0)\n",
    "        close_paren_id = self.smiles_tokenizer.word_index.get(')', 0)\n",
    "        \n",
    "        # Count open and close parentheses in predicted sequence\n",
    "        open_count = tf.reduce_sum(tf.cast(tf.equal(char_indices, open_paren_id), tf.float32), axis=1)\n",
    "        close_count = tf.reduce_sum(tf.cast(tf.equal(char_indices, close_paren_id), tf.float32), axis=1)\n",
    "        \n",
    "        # Penalty for unbalanced parentheses\n",
    "        paren_penalty = tf.abs(open_count - close_count) * 0.1\n",
    "        \n",
    "        # 2. Ring number balance penalty\n",
    "        ring_penalties = []\n",
    "        for num in range(1, 10):  # Ring numbers 1-9\n",
    "            num_id = self.smiles_tokenizer.word_index.get(str(num), 0)\n",
    "            if num_id > 0:\n",
    "                count = tf.reduce_sum(tf.cast(tf.equal(char_indices, num_id), tf.float32), axis=1)\n",
    "                # Each ring number should appear an even number of times\n",
    "                ring_penalties.append(tf.math.floormod(count, 2.0) * 0.05)\n",
    "        \n",
    "        ring_penalty = tf.add_n(ring_penalties) if ring_penalties else tf.zeros_like(paren_penalty)\n",
    "    \n",
    "        # 3. Starting/ending character penalties\n",
    "        first_chars = char_indices[:, 0]\n",
    "        last_chars = char_indices[:, -1]\n",
    "        \n",
    "        # Get token IDs for characters that shouldn't start SMILES\n",
    "        bad_start_ids = [self.smiles_tokenizer.word_index.get(c, 0) \n",
    "                        for c in [')', '=', '#', '+', '-'] if c in self.smiles_tokenizer.word_index]\n",
    "        bad_end_ids = [self.smiles_tokenizer.word_index.get(c, 0)\n",
    "                    for c in ['(', '=', '#'] if c in self.smiles_tokenizer.word_index]\n",
    "        \n",
    "        bad_start_penalty = tf.zeros_like(paren_penalty)\n",
    "        bad_end_penalty = tf.zeros_like(paren_penalty)\n",
    "        \n",
    "        # Add penalty for each bad start/end character\n",
    "        for bad_id in bad_start_ids:\n",
    "            bad_start_penalty += tf.cast(tf.equal(first_chars, bad_id), tf.float32) * 0.1\n",
    "        \n",
    "        for bad_id in bad_end_ids:\n",
    "            bad_end_penalty += tf.cast(tf.equal(last_chars, bad_id), tf.float32) * 0.1\n",
    "        \n",
    "        # Combine all penalties\n",
    "        total_penalty = paren_penalty + ring_penalty + bad_start_penalty + bad_end_penalty\n",
    "        \n",
    "        # Shape the penalty to match the loss shape\n",
    "        return total_penalty\n",
    "    \n",
    "        \n",
    "    def build_cvae(self):\n",
    "        \"\"\"Build the complete CVAE model with proper Keras layers and SMILES validity loss.\"\"\"\n",
    "        # Build encoder and decoder\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        \n",
    "        # CVAE inputs\n",
    "        smiles_input = keras.Input(shape=(self.smiles_max_length,), name='smiles_input')\n",
    "        target_input = keras.Input(shape=(self.target_max_length,), name='target_input')\n",
    "        affinity_input = keras.Input(shape=(1,), name='affinity_input')\n",
    "        \n",
    "        # Get latent space representation\n",
    "        z_mean, z_log_var, z = self.encoder([smiles_input, target_input, affinity_input])\n",
    "        \n",
    "        # Apply KL divergence (this layer adds the KL loss to the model)\n",
    "        kl_layer = KLDivergenceLayer()\n",
    "        kl_layer([z_mean, z_log_var])\n",
    "        \n",
    "        # Generate SMILES from latent space\n",
    "        smiles_output = self.decoder([z, target_input, affinity_input])\n",
    "\n",
    "        # Create CVAE model\n",
    "        self.cvae = keras.Model(\n",
    "            inputs=[smiles_input, target_input, affinity_input],\n",
    "            outputs=smiles_output,\n",
    "            name='cvae'\n",
    "        )\n",
    "        \n",
    "        # Define custom loss that combines categorical crossentropy with SMILES validity loss\n",
    "        def combined_loss(y_true, y_pred):\n",
    "            # Regular categorical crossentropy\n",
    "            ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "            ce_loss = tf.reduce_mean(ce_loss)\n",
    "            \n",
    "            # Add SMILES validity loss\n",
    "            validity_loss = self.smiles_validity_loss(y_true, y_pred)\n",
    "            validity_loss = tf.reduce_mean(validity_loss)\n",
    "            \n",
    "            # Cast both to the same type before adding\n",
    "            ce_loss = tf.cast(ce_loss, tf.float32)\n",
    "            validity_loss = tf.cast(validity_loss, tf.float32)\n",
    "            \n",
    "            # Return combined loss with weighting\n",
    "            return ce_loss + 0.5 * validity_loss\n",
    "    \n",
    "        # Compile with the combined loss\n",
    "        self.cvae.compile(\n",
    "            optimizer='adam',\n",
    "            loss=combined_loss\n",
    "        )\n",
    "        \n",
    "        # Print model summaries\n",
    "        print(\"\\n---- Encoder Model Summary ----\")\n",
    "        self.encoder.summary()\n",
    "        \n",
    "        print(\"\\n---- Decoder Model Summary ----\")\n",
    "        self.decoder.summary()\n",
    "        \n",
    "        print(\"\\n---- Complete CVAE Model Summary ----\")\n",
    "        self.cvae.summary()\n",
    "        \n",
    "        # Calculate total parameters\n",
    "        encoder_params = self.encoder.count_params()\n",
    "        decoder_params = self.decoder.count_params()\n",
    "        total_params = self.cvae.count_params()\n",
    "        \n",
    "        print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "        print(f\"Encoder parameters: {encoder_params:,}\")\n",
    "        print(f\"Decoder parameters: {decoder_params:,}\")\n",
    "        \n",
    "        print(\"CVAE model built and compiled with SMILES validity loss.\")\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def train(self, train_data, val_data, batch_size=32, epochs=50):\n",
    "        \"\"\"Train the CVAE model.\"\"\"\n",
    "        if self.cvae is None:\n",
    "            self.build_cvae()\n",
    "        \n",
    "        print(f\"Training CVAE for {epochs} epochs with batch size {batch_size}...\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath=os.path.join(self.models_dir, 'cvae_best.h5'),\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "        # Prepare simplified inputs (we don't need smiles_one_hot as input anymore)\n",
    "        train_inputs = [\n",
    "            train_data['smiles_data'],      # Integer encoded SMILES\n",
    "            train_data['target_data'],      # Target protein data\n",
    "            train_data['affinities']        # Affinity values\n",
    "        ]\n",
    "        \n",
    "        val_inputs = [\n",
    "            val_data['smiles_data'],\n",
    "            val_data['target_data'],\n",
    "            val_data['affinities']\n",
    "        ]\n",
    "        \n",
    "        # Train the model\n",
    "        self.history = self.cvae.fit(\n",
    "            train_inputs,\n",
    "            train_data['smiles_one_hot'],  # Target is the one-hot SMILES\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(val_inputs, val_data['smiles_one_hot']),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # Save models\n",
    "        self.cvae.save(os.path.join(self.models_dir, 'cvae_final.h5'))\n",
    "        self.encoder.save(os.path.join(self.models_dir, 'encoder.h5'))\n",
    "        self.decoder.save(os.path.join(self.models_dir, 'decoder.h5'))\n",
    "        \n",
    "        print(\"Training completed. Models saved.\")\n",
    "        self.plot_training_history()\n",
    "        \n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot the training history.\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available.\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Plot training & validation loss\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        plt.plot(self.history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(self.models_dir, 'training_history.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Training history plotted and saved.\")\n",
    "\n",
    "    def generate_molecules(self, target_sequence, desired_affinity, n_samples=20, temperature=0.6):\n",
    "        \"\"\"Generate valid molecules with enhanced grammar constraints for SMILES.\"\"\"\n",
    "        if self.decoder is None:\n",
    "            raise ValueError(\"Decoder model not available. Train or load a model first.\")\n",
    "        \n",
    "        # Scale affinity\n",
    "        scaled_affinity = self.affinity_scaler.transform([[desired_affinity]])\n",
    "        \n",
    "        # Convert target sequence to input format\n",
    "        target_seq = self.target_tokenizer.texts_to_sequences([target_sequence])\n",
    "        padded_target = pad_sequences(target_seq, maxlen=self.target_max_length, padding='post')\n",
    "        \n",
    "        # Create reverse mapping from indices to characters\n",
    "        idx_to_char = {idx: char for char, idx in self.smiles_tokenizer.word_index.items()}\n",
    "        idx_to_char[0] = ''  # Add padding token\n",
    "        \n",
    "        # Generate many more samples to improve chances of getting valid molecules\n",
    "        n_attempts = max(200, n_samples * 20)  # Increased from 10x to 20x\n",
    "        z_samples = np.random.normal(size=(n_attempts, self.latent_dim))\n",
    "        \n",
    "        # Prepare conditional inputs\n",
    "        target_input = np.repeat(padded_target, n_attempts, axis=0)\n",
    "        affinity_input = np.repeat(scaled_affinity, n_attempts, axis=0)\n",
    "        \n",
    "        # Generate probabilities\n",
    "        print(f\"Generating {n_attempts} candidate molecules...\")\n",
    "        generated = self.decoder.predict([z_samples, target_input, affinity_input], verbose=1)\n",
    "        \n",
    "        # Track valid SMILES and all attempts\n",
    "        valid_smiles = []\n",
    "        all_attempts = 0\n",
    "        progress_interval = max(1, n_attempts // 10)\n",
    "        \n",
    "        # Get character IDs grouped by type\n",
    "        atom_ids = [self.smiles_tokenizer.word_index.get(c, 0) for c in 'CNOPSFIBrclnops']\n",
    "        branch_open_ids = [self.smiles_tokenizer.word_index.get('(', 0)]\n",
    "        branch_close_ids = [self.smiles_tokenizer.word_index.get(')', 0)]\n",
    "        ring_ids = [self.smiles_tokenizer.word_index.get(str(i), 0) for i in range(1,10)]\n",
    "        bond_ids = [self.smiles_tokenizer.word_index.get(c, 0) for c in '-=#:']\n",
    "        \n",
    "        for i in range(n_attempts):\n",
    "            if len(valid_smiles) >= n_samples:\n",
    "                break\n",
    "        \n",
    "            all_attempts += 1\n",
    "            if all_attempts % progress_interval == 0:\n",
    "                print(f\"Processing attempt {all_attempts}/{n_attempts}, found {len(valid_smiles)} valid molecules...\")\n",
    "            \n",
    "            # Process this candidate with adjusted temperature to control randomness\n",
    "            probs = np.exp(np.log(generated[i] + 1e-10) / temperature)\n",
    "            probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "            \n",
    "            # State tracking for grammar constraints\n",
    "            open_branches = 0  # Count of open parentheses\n",
    "            open_rings = set()  # Set of currently open ring numbers\n",
    "            last_was_atom = False\n",
    "            last_was_bond = False\n",
    "            last_was_ring = False\n",
    "            last_atom = None\n",
    "            \n",
    "            smiles_indices = []\n",
    "            \n",
    "            # Build molecule token by token with strict constraints\n",
    "            for j in range(min(self.smiles_max_length, probs.shape[0])):\n",
    "                # For first position, must be atom or open branch\n",
    "                if j == 0:\n",
    "                    allowed_ids = atom_ids\n",
    "                    mask = np.zeros_like(probs[j])\n",
    "                    for idx in allowed_ids:\n",
    "                        if idx < len(mask):\n",
    "                            mask[idx] = 1\n",
    "                    filtered_probs = probs[j] * mask\n",
    "                \n",
    "                else:  # Use grammar rules for subsequent positions\n",
    "                    mask = np.zeros_like(probs[j])\n",
    "                    \n",
    "                    # Rules based on previous token type\n",
    "                    if last_was_atom:\n",
    "                        # After atom: can be bond, branch start/end, ring number, or another atom\n",
    "                        allowed_ids = bond_ids + branch_open_ids + branch_close_ids + ring_ids + atom_ids\n",
    "                        # Don't allow branch close if no branches are open\n",
    "                        if open_branches == 0:\n",
    "                            allowed_ids = [id for id in allowed_ids if id not in branch_close_ids]\n",
    "                        \n",
    "                    elif last_was_bond:\n",
    "                        # After bond: must be atom or branch open\n",
    "                        allowed_ids = atom_ids + branch_open_ids\n",
    "                        \n",
    "                    elif last_was_ring:\n",
    "                        # After ring: can be bond, branch start/end, another ring, or atom\n",
    "                        allowed_ids = bond_ids + branch_open_ids + branch_close_ids + ring_ids + atom_ids\n",
    "                        if open_branches == 0:\n",
    "                            allowed_ids = [id for id in allowed_ids if id not in branch_close_ids]\n",
    "                        \n",
    "                    else:  # After branch open/close\n",
    "                        if open_branches > 0:\n",
    "                            # After branch open: must be atom, bond or another branch open\n",
    "                            allowed_ids = atom_ids + bond_ids + branch_open_ids\n",
    "                        else:\n",
    "                            # After a full branch closure (no open branches), can be almost anything\n",
    "                            allowed_ids = atom_ids + bond_ids + branch_open_ids\n",
    "                    \n",
    "                    # Apply mask for allowed tokens\n",
    "                    for idx in allowed_ids:\n",
    "                        if idx < len(mask):\n",
    "                            mask[idx] = 1\n",
    "                    filtered_probs = probs[j] * mask\n",
    "                \n",
    "                # If no allowed tokens (all zeros), default to carbon\n",
    "                if np.sum(filtered_probs) == 0:\n",
    "                    carbon_id = self.smiles_tokenizer.word_index.get('C', 0)\n",
    "                    if carbon_id > 0:\n",
    "                        idx = carbon_id\n",
    "                    else:\n",
    "                        break  # Can't continue if we don't have a carbon atom ID\n",
    "                else:\n",
    "                    # Re-normalize probabilities\n",
    "                    filtered_probs = filtered_probs / np.sum(filtered_probs)\n",
    "                \n",
    "                    # Sample token from filtered probabilities\n",
    "                    try:\n",
    "                        idx = np.random.choice(len(filtered_probs), p=filtered_probs)\n",
    "                    except:\n",
    "                        # Fallback if sampling fails\n",
    "                        idx = np.argmax(filtered_probs)\n",
    "                \n",
    "                # Update state based on selected token\n",
    "                char = idx_to_char.get(idx, '')\n",
    "                \n",
    "                # Update state tracking based on the chosen token\n",
    "                if char in 'CNOPSFIBrclnops':\n",
    "                    last_was_atom = True\n",
    "                    last_was_bond = False\n",
    "                    last_was_ring = False\n",
    "                    last_atom = char\n",
    "                elif char in '-=#:':\n",
    "                    last_was_atom = False\n",
    "                    last_was_bond = True\n",
    "                    last_was_ring = False\n",
    "                elif char in '123456789':\n",
    "                    last_was_atom = False\n",
    "                    last_was_bond = False\n",
    "                    last_was_ring = True\n",
    "                    # Toggle ring open/closed status\n",
    "                    if char in open_rings:\n",
    "                        open_rings.remove(char)\n",
    "                    else:\n",
    "                        open_rings.add(char)\n",
    "                elif char == '(':\n",
    "                    open_branches += 1\n",
    "                    last_was_atom = False\n",
    "                    last_was_bond = False\n",
    "                    last_was_ring = False\n",
    "                elif char == ')':\n",
    "                    if open_branches > 0:  # Only close if we have open branches\n",
    "                        open_branches -= 1\n",
    "                    last_was_atom = False\n",
    "                    last_was_bond = False\n",
    "                    last_was_ring = False\n",
    "            \n",
    "                if char:  # Add non-empty character to our sequence\n",
    "                    smiles_indices.append(idx)\n",
    "                \n",
    "                # Early stopping conditions:\n",
    "                # 1. When we have a reasonable length and no open branches/rings\n",
    "                # 2. Or if we hit a max absolute length\n",
    "                reasonable_length = len(smiles_indices) >= 5\n",
    "                completed = open_branches == 0 and len(open_rings) == 0\n",
    "                if reasonable_length and completed and last_was_atom:\n",
    "                    break\n",
    "                if len(smiles_indices) >= self.smiles_max_length * 0.75:\n",
    "                    break\n",
    "            \n",
    "            # Convert indices to SMILES string\n",
    "            generated_smiles = ''.join([idx_to_char.get(idx, '') for idx in smiles_indices])\n",
    "            \n",
    "            # Clean up any remaining issues and ensure valid chemistry\n",
    "            processed_smiles = self._fix_smiles(generated_smiles)\n",
    "            \n",
    "            # Validate with RDKit\n",
    "            mol = Chem.MolFromSmiles(processed_smiles)\n",
    "            if mol is not None:\n",
    "                # Get canonical SMILES to avoid duplicates\n",
    "                canonical_smiles = Chem.MolToSmiles(mol)\n",
    "                if canonical_smiles not in valid_smiles:\n",
    "                    valid_smiles.append(canonical_smiles)\n",
    "        \n",
    "        print(f\"Generated {len(valid_smiles)} valid molecules out of {all_attempts} attempts.\")\n",
    "        \n",
    "        # Return the requested number of molecules (or all if fewer were found)\n",
    "        return valid_smiles[:n_samples]\n",
    "        \n",
    "        \n",
    "                    \n",
    "                   \n",
    "\n",
    "    def _fix_smiles(self, smiles):\n",
    "        \"\"\"Enhanced function to fix common SMILES issues\"\"\"\n",
    "        if not smiles:\n",
    "            return \"C\"  # Default to methane if empty\n",
    "        \n",
    "        # Clean up invalid characters\n",
    "        valid_chars = set('CNOPSFIBrclnops-=#:()[]123456789.+%')\n",
    "        smiles = ''.join(c for c in smiles if c in valid_chars)\n",
    "        \n",
    "        # Ensure there's at least one atom\n",
    "        if not any(c.upper() in 'CNOPSFIBRCLNOPS' for c in smiles):\n",
    "            return \"C\"  # Default to methane if no valid atoms\n",
    "        \n",
    "        # Step 1: Fix basic structure issues\n",
    "        try:\n",
    "            # Find and process valid fragment if SMILES is invalid\n",
    "            for attempt in range(3):  # Try multiple strategies\n",
    "                # Balance parentheses\n",
    "                open_count = smiles.count('(') + smiles.count('[')\n",
    "                close_count = smiles.count(')') + smiles.count(']')\n",
    "                \n",
    "                if open_count > close_count:\n",
    "                    smiles += ')' * (open_count - close_count)\n",
    "                elif close_count > open_count:\n",
    "                    smiles = '(' * (close_count - open_count) + smiles\n",
    "            \n",
    "                # Fix ring numbers (each must appear exactly twice)\n",
    "                for num in '123456789':\n",
    "                    count = smiles.count(num)\n",
    "                    if count % 2 == 1:  # Odd count needs fixing\n",
    "                        if count == 1:  # If only appears once, remove it\n",
    "                            smiles = smiles.replace(num, '')\n",
    "                        else:  # For higher odd counts, remove last occurrence\n",
    "                            last_idx = smiles.rindex(num)\n",
    "                            smiles = smiles[:last_idx] + smiles[last_idx+1:]\n",
    "                \n",
    "                # Fix bond issues (no consecutive bonds, no trailing bonds)\n",
    "                i = 0\n",
    "                while i < len(smiles) - 1:\n",
    "                    if smiles[i] in '-=#:' and smiles[i+1] in '-=#:':\n",
    "                        smiles = smiles[:i+1] + 'C' + smiles[i+1:]\n",
    "                    i += 1\n",
    "                \n",
    "                # Handle bonds at beginning/end\n",
    "                if smiles and smiles[0] in '-=#:':\n",
    "                    smiles = 'C' + smiles\n",
    "                if smiles and smiles[-1] in '-=#:':\n",
    "                    smiles = smiles + 'C'\n",
    "            \n",
    "                # Try to parse with RDKit\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    return Chem.MolToSmiles(mol)  # Return canonical form\n",
    "                \n",
    "                # If parsing failed, try simpler fragment-based approach\n",
    "                if attempt == 1:\n",
    "                    # Find the largest sequence of valid atoms/bonds\n",
    "                    atoms = set('CNOPSFIBrclnops')\n",
    "                    valid_fragment = \"\"\n",
    "                    current_fragment = \"\"\n",
    "                    \n",
    "                    for c in smiles:\n",
    "                        if c in atoms or c in '-=#:123456789()':\n",
    "                            current_fragment += c\n",
    "                        else:\n",
    "                            if len(current_fragment) > len(valid_fragment):\n",
    "                                valid_fragment = current_fragment\n",
    "                            current_fragment = \"\"\n",
    "                \n",
    "                    if len(current_fragment) > len(valid_fragment):\n",
    "                        valid_fragment = current_fragment\n",
    "                    \n",
    "                    if valid_fragment:\n",
    "                        smiles = valid_fragment\n",
    "                    else:\n",
    "                        return \"C\"  # If no valid fragment, return methane\n",
    "                \n",
    "                # Last resort: just keep atoms and simple bonds\n",
    "                if attempt == 2:\n",
    "                    atoms = ''.join(c for c in smiles if c.upper() in 'CNOPSFIBRCLNOPS')\n",
    "                    if atoms:\n",
    "                        return atoms[0].upper()  # Return first atom\n",
    "                    return \"C\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            # If any exception occurs in fixing, return a simple structure\n",
    "            print(f\"Error fixing SMILES: {e}\")\n",
    "            return \"C\"\n",
    "        \n",
    "        # If all else fails, return methane\n",
    "        return \"C\"\n",
    "        \n",
    "    \n",
    "              \n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "    def save_preprocessing_objects(self, filepath=None):\n",
    "        \"\"\"Save tokenizers and scalers for later use\"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = self.models_dir\n",
    "        \n",
    "        # Save tokenizers\n",
    "        with open(os.path.join(filepath, 'smiles_tokenizer.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.smiles_tokenizer, f)\n",
    "        \n",
    "        with open(os.path.join(filepath, 'target_tokenizer.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.target_tokenizer, f)\n",
    "        \n",
    "        # Save affinity scaler\n",
    "        with open(os.path.join(filepath, 'affinity_scaler.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.affinity_scaler, f)\n",
    "        \n",
    "        # Save vocabulary sizes\n",
    "        with open(os.path.join(filepath, 'vocab_sizes.json'), 'w') as f:\n",
    "            json.dump({\n",
    "                'smiles_vocab_size': self.smiles_vocab_size,\n",
    "                'target_vocab_size': self.target_vocab_size\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"Preprocessing objects saved to {filepath}\")\n",
    "\n",
    "    def load_preprocessing_objects(self, filepath=None):\n",
    "        \"\"\"Load tokenizers and scalers\"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = self.models_dir\n",
    "        \n",
    "        # Load tokenizers\n",
    "        with open(os.path.join(filepath, 'smiles_tokenizer.pkl'), 'rb') as f:\n",
    "            self.smiles_tokenizer = pickle.load(f)\n",
    "        \n",
    "        with open(os.path.join(filepath, 'target_tokenizer.pkl'), 'rb') as f:\n",
    "            self.target_tokenizer = pickle.load(f)\n",
    "        \n",
    "        # Load affinity scaler\n",
    "        with open(os.path.join(filepath, 'affinity_scaler.pkl'), 'rb') as f:\n",
    "            self.affinity_scaler = pickle.load(f)\n",
    "        \n",
    "        # Load vocabulary sizes\n",
    "        with open(os.path.join(filepath, 'vocab_sizes.json'), 'r') as f:\n",
    "            vocab_sizes = json.load(f)\n",
    "            self.smiles_vocab_size = vocab_sizes['smiles_vocab_size']\n",
    "            self.target_vocab_size = vocab_sizes['target_vocab_size']\n",
    "        \n",
    "        print(f\"Preprocessing objects loaded from {filepath}\")\n",
    "\n",
    "    def _process_data_chunk(self, chunk_df):\n",
    "        \"\"\"Process a chunk of data\"\"\"\n",
    "        # Convert sequences to integer indices\n",
    "        smiles_seqs = self.smiles_tokenizer.texts_to_sequences(chunk_df['Drug'].values)\n",
    "        target_seqs = self.target_tokenizer.texts_to_sequences(chunk_df['Target'].values)\n",
    "        \n",
    "        # Pad sequences\n",
    "        smiles_data = pad_sequences(smiles_seqs, maxlen=self.smiles_max_length, padding='post')\n",
    "        target_data = pad_sequences(target_seqs, maxlen=self.target_max_length, padding='post')\n",
    "        \n",
    "        # One-hot encode SMILES\n",
    "        smiles_one_hot = tf.keras.utils.to_categorical(smiles_data, num_classes=self.smiles_vocab_size)\n",
    "        \n",
    "        # Scale affinities\n",
    "        affinities = chunk_df['Affinity'].values.reshape(-1, 1)\n",
    "        scaled_affinities = self.affinity_scaler.transform(affinities)\n",
    "        \n",
    "        # Split into train/val\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(chunk_df)), test_size=0.1, random_state=42)\n",
    "        \n",
    "        train_chunk = {\n",
    "            'smiles_data': smiles_data[train_indices],\n",
    "            'smiles_one_hot': smiles_one_hot[train_indices],\n",
    "            'target_data': target_data[train_indices],\n",
    "            'affinities': scaled_affinities[train_indices]\n",
    "        }\n",
    "        \n",
    "        val_chunk = {\n",
    "            'smiles_data': smiles_data[val_indices],\n",
    "            'smiles_one_hot': smiles_one_hot[val_indices],\n",
    "            'target_data': target_data[val_indices],\n",
    "            'affinities': scaled_affinities[val_indices]\n",
    "        }\n",
    "        \n",
    "        return train_chunk, val_chunk\n",
    "\n",
    "    def _combine_data_chunks(self, chunks):\n",
    "        \"\"\"Combine data chunks\"\"\"\n",
    "        combined = {\n",
    "            'smiles_data': np.vstack([chunk['smiles_data'] for chunk in chunks]),\n",
    "            'smiles_one_hot': np.vstack([chunk['smiles_one_hot'] for chunk in chunks]),\n",
    "            'target_data': np.vstack([chunk['target_data'] for chunk in chunks]),\n",
    "            'affinities': np.vstack([chunk['affinities'] for chunk in chunks])\n",
    "        }\n",
    "        return combined\n",
    "    \n",
    "    def train_with_checkpoint(self, df, batch_size=32, epochs_per_checkpoint=5, \n",
    "                         max_epochs=100, val_split=0.2, resume_from=None,\n",
    "                         save_every=5):\n",
    "        \"\"\"Train on large datasets with checkpointing using available accelerators.\"\"\"\n",
    "        print(\"Preparing for training with checkpointing...\")\n",
    "        \n",
    "        # Check for available accelerators\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        tpus = tf.config.list_physical_devices('TPU')\n",
    "    \n",
    "        if gpus:\n",
    "            print(f\"Found {len(gpus)} GPU(s). Configuring TensorFlow to use GPU acceleration.\")\n",
    "            try:\n",
    "                # Enable memory growth to avoid allocating all GPU memory at once\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                \n",
    "                # Enable mixed precision training for faster computation on modern GPUs\n",
    "                if tf.__version__ >= '2.4.0':\n",
    "                    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "                    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "                    print(\"Enabled mixed precision training (float16)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error configuring GPU: {e}\")\n",
    "        elif tpus:\n",
    "            print(f\"Found {len(tpus)} TPU(s). Configuring TensorFlow to use TPU acceleration.\")\n",
    "            try:\n",
    "                resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "                tf.config.experimental_connect_to_cluster(resolver)\n",
    "                tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "                strategy = tf.distribute.TPUStrategy(resolver)\n",
    "                print(f\"TPU strategy initialized with {strategy.num_replicas_in_sync} replicas\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error configuring TPU: {e}\")\n",
    "        else:\n",
    "            print(\"No GPUs or TPUs found. Training will proceed on CPU.\")\n",
    "        \n",
    "        # Force eager execution for better error messages and debugging\n",
    "        tf.config.run_functions_eagerly(True)\n",
    "\n",
    "        if len(df) > 50000:\n",
    "            print(f\"Large dataset detected ({len(df)} samples). Using sampling strategy.\")\n",
    "            # Sample a subset for preprocessing fitting\n",
    "            sample_size = min(50000, len(df) // 2)\n",
    "            fit_sample = df.sample(sample_size, random_state=42)\n",
    "            \n",
    "            # Fit tokenizers on the sample\n",
    "            self.smiles_tokenizer = Tokenizer(char_level=True)\n",
    "            self.smiles_tokenizer.fit_on_texts(fit_sample['Drug'].values)\n",
    "            self.smiles_vocab_size = len(self.smiles_tokenizer.word_index) + 1\n",
    "            \n",
    "            self.target_tokenizer = Tokenizer(char_level=True)\n",
    "            self.target_tokenizer.fit_on_texts(fit_sample['Target'].values)\n",
    "            self.target_vocab_size = len(self.target_tokenizer.word_index) + 1\n",
    "            \n",
    "            # Fit the affinity scaler on the sample\n",
    "            self.affinity_scaler.fit(fit_sample['Affinity'].values.reshape(-1, 1))\n",
    "            \n",
    "            # Now process in smaller chunks\n",
    "            chunk_size = 10000\n",
    "            train_chunks = []\n",
    "            val_chunks = []\n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                chunk = df.iloc[i:i+chunk_size]\n",
    "                chunk_train, chunk_val = self._process_data_chunk(chunk)\n",
    "                train_chunks.append(chunk_train)\n",
    "                val_chunks.append(chunk_val)\n",
    "            \n",
    "            # Combine the chunks\n",
    "            train_data = self._combine_data_chunks(train_chunks)\n",
    "            val_data = self._combine_data_chunks(val_chunks)\n",
    "        \n",
    "        if resume_from:\n",
    "            # Load previous preprocessing and weights\n",
    "            self.load_preprocessing_objects(resume_from)\n",
    "            starting_epoch = self._load_checkpoint(resume_from)\n",
    "            print(f\"Resuming from epoch {starting_epoch}\")\n",
    "        else:\n",
    "            # Start from scratch - preprocess data\n",
    "            train_data, val_data = self.preprocess_existing_dataframe(df)\n",
    "            \n",
    "            # Save preprocessing objects\n",
    "            self.save_preprocessing_objects()\n",
    "            \n",
    "            # Build model if not already built\n",
    "            if self.cvae is None:\n",
    "                self.build_cvae()\n",
    "            \n",
    "            starting_epoch = 0\n",
    "        \n",
    "        # Callbacks for training\n",
    "        checkpoint_dir = os.path.join(self.models_dir, 'checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=os.path.join(checkpoint_dir, 'epoch{epoch:03d}.weights.h5'),\n",
    "                save_weights_only=True,\n",
    "                save_freq=save_every * (len(train_data['smiles_data']) // batch_size),\n",
    "                save_best_only=False\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=os.path.join(checkpoint_dir, 'best_model.weights.h5'),\n",
    "                save_weights_only=True,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True\n",
    "            ),    \n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=os.path.join(self.models_dir, 'logs'),\n",
    "                histogram_freq=1,\n",
    "                write_graph=True,\n",
    "                profile_batch='500,520'  # Profile performance for specific batches\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger(\n",
    "                os.path.join(self.models_dir, 'training_log.csv'),\n",
    "                append=True if resume_from else False\n",
    "            ),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Convert inputs to numpy arrays with explicit dtypes for better performance\n",
    "        train_inputs = [\n",
    "            tf.convert_to_tensor(train_data['smiles_data'], dtype=tf.float16),\n",
    "            tf.convert_to_tensor(train_data['target_data'], dtype=tf.float16),\n",
    "            tf.convert_to_tensor(train_data['affinities'], dtype=tf.float16)\n",
    "        ]\n",
    "        \n",
    "        val_inputs = [\n",
    "            np.array(val_data['smiles_data'], dtype=np.float32),\n",
    "            np.array(val_data['target_data'], dtype=np.float32),\n",
    "            np.array(val_data['affinities'], dtype=np.float32)\n",
    "        ]\n",
    "        \n",
    "        train_outputs = np.array(train_data['smiles_one_hot'], dtype=np.float32)\n",
    "        val_outputs = np.array(val_data['smiles_one_hot'], dtype=np.float32)\n",
    "        \n",
    "        # Train with error handling and fallback options\n",
    "        print(f\"Starting training from epoch {starting_epoch} for {max_epochs} epochs\")\n",
    "        try:\n",
    "            # Use a larger batch size if GPU is available\n",
    "            if gpus:\n",
    "                adjusted_batch_size = batch_size * 2\n",
    "                print(f\"Using increased batch size for GPU: {adjusted_batch_size}\")\n",
    "            else:\n",
    "                adjusted_batch_size = batch_size\n",
    "            \n",
    "            # FIXED: Removed unsupported 'workers' and 'use_multiprocessing' arguments\n",
    "            self.history = self.cvae.fit(\n",
    "                train_inputs,\n",
    "                train_outputs,\n",
    "                batch_size=adjusted_batch_size,\n",
    "                initial_epoch=starting_epoch,\n",
    "                epochs=starting_epoch + max_epochs,\n",
    "                validation_data=(val_inputs, val_outputs),\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "                # Removed: workers=4, use_multiprocessing=True\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during training: {e}\")\n",
    "            print(\"Trying again with CPU only...\")\n",
    "            \n",
    "            \n",
    "            # Reset mixed precision policy if it was enabled\n",
    "            if gpus and tf.__version__ >= '2.4.0':\n",
    "                tf.keras.mixed_precision.set_global_policy('float32')\n",
    "            \n",
    "            # Fall back to CPU-only execution\n",
    "            with tf.device('/CPU:0'):\n",
    "                self.history = self.cvae.fit(\n",
    "                    train_inputs,\n",
    "                    train_outputs,\n",
    "                    batch_size=batch_size,\n",
    "                    initial_epoch=starting_epoch,\n",
    "                    epochs=starting_epoch + max_epochs,\n",
    "                    validation_data=(val_inputs, val_outputs),\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1\n",
    "                )\n",
    "        \n",
    "        # Reset to graph mode for inference\n",
    "        tf.config.run_functions_eagerly(False)\n",
    "        \n",
    "        # FIXED: Changed file extensions to match required pattern (.weights.h5)\n",
    "        self.cvae.save_weights(os.path.join(self.models_dir, 'cvae_final.weights.h5'))\n",
    "        self.encoder.save_weights(os.path.join(self.models_dir, 'encoder_final.weights.h5'))\n",
    "        self.decoder.save_weights(os.path.join(self.models_dir, 'decoder_final.weights.h5'))\n",
    "        \n",
    "        print(\"Training completed. Models saved.\")\n",
    "        self.plot_training_history()\n",
    "\n",
    "    def _load_checkpoint(self, checkpoint_dir):\n",
    "        \"\"\"Load model weights from checkpoint\"\"\"\n",
    "        # Find the latest checkpoint with the .weights.h5 extension\n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) \n",
    "                    if f.startswith('epoch') and f.endswith('.weights.h5')]\n",
    "        \n",
    "        if not checkpoints:\n",
    "            print(\"No checkpoints found.\")\n",
    "            return 0\n",
    "        \n",
    "        # Get the latest checkpoint\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x[5:8]))  # Extract epoch number\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "        \n",
    "        # Load weights\n",
    "        self.cvae.load_weights(checkpoint_path)\n",
    "        \n",
    "        # Extract epoch number from checkpoint name\n",
    "        epoch = int(latest_checkpoint[5:8])\n",
    "        return epoch + 1\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generate batches of data on the fly to save memory\"\"\"\n",
    "    \n",
    "    def __init__(self, smiles_data, target_data, affinities, smiles_one_hot,  \n",
    "                 batch_size=32, shuffle=True):\n",
    "        self.smiles_data = smiles_data\n",
    "        self.target_data = target_data\n",
    "        self.affinities = affinities\n",
    "        self.smiles_one_hot = smiles_one_hot\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.smiles_data))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.smiles_data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Create batch\n",
    "        batch_smiles = self.smiles_data[batch_indices]\n",
    "        batch_target = self.target_data[batch_indices]\n",
    "        batch_affinity = self.affinities[batch_indices]\n",
    "        batch_smiles_one_hot = self.smiles_one_hot[batch_indices]\n",
    "        \n",
    "        # Create input and output\n",
    "        x = [batch_smiles, batch_target, batch_affinity]\n",
    "        y = batch_smiles_one_hot\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "class DataChunkGenerator:\n",
    "    \"\"\"Process and train on data in chunks to avoid memory issues\"\"\"\n",
    "    \n",
    "    def __init__(self, df, chunk_size=1000):\n",
    "        self.df = df\n",
    "        self.chunk_size = chunk_size\n",
    "        self.current_idx = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(range(0, len(self.df), self.chunk_size))\n",
    "    \n",
    "    def get_next_chunk(self):\n",
    "        \"\"\"Get next chunk of data\"\"\"\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return None\n",
    "        \n",
    "        end_idx = min(self.current_idx + self.chunk_size, len(self.df))\n",
    "        chunk = self.df.iloc[self.current_idx:end_idx]\n",
    "        self.current_idx = end_idx\n",
    "        return chunk\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset to beginning of data\"\"\"\n",
    "        self.current_idx = 0\n",
    "\n",
    "# Example usage with Kaggle\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    \n",
    "    chunk_generator = DataChunkGenerator(df, chunk_size=5000)\n",
    "    model = DrugDiscoveryCVAE(\n",
    "        smiles_max_length=100,\n",
    "        target_max_length=512,\n",
    "        latent_dim=16\n",
    "    )\n",
    "    \n",
    "    # First, fit tokenizers on a sample of data\n",
    "    sample_size = min(50000, len(df))\n",
    "    sample_df = df.sample(sample_size, random_state=42)\n",
    "    train_data, val_data = model.preprocess_existing_dataframe(sample_df)\n",
    "    \n",
    "    # Build model\n",
    "    model.build_cvae()\n",
    "    # Then train on chunks\n",
    "    for epoch in range(50):\n",
    "        print(f\"Epoch {epoch+1}/50\")\n",
    "        chunk_generator.reset()\n",
    "        epoch_loss = []\n",
    "        \n",
    "        chunk = chunk_generator.get_next_chunk()\n",
    "        chunk_num = 1\n",
    "        \n",
    "        while chunk is not None:\n",
    "            print(f\"  Processing chunk {chunk_num}/{len(chunk_generator)}...\")\n",
    "            \n",
    "            # Process chunk\n",
    "            chunk_train, chunk_val = model._process_data_chunk(chunk)\n",
    "            \n",
    "            # Train on chunk with reduced batch size\n",
    "            batch_size = 16  # Very small batch size\n",
    "            history = model.cvae.fit(\n",
    "                [chunk_train['smiles_data'], chunk_train['target_data'], chunk_train['affinities']], \n",
    "                chunk_train['smiles_one_hot'],\n",
    "                batch_size=batch_size,\n",
    "                epochs=1,\n",
    "                verbose=1\n",
    "            )\n",
    "            epoch_loss.append(history.history['loss'][0])\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Get next chunk\n",
    "            chunk = chunk_generator.get_next_chunk()\n",
    "            chunk_num += 1\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        os.makedirs(\"models/checkpoints\", exist_ok=True)\n",
    "\n",
    "        model.cvae.save_weights(f\"models/checkpoints/epoch{epoch+1:03d}.weights.h5\")\n",
    "        print(f\"Epoch {epoch+1} average loss: {sum(epoch_loss)/len(epoch_loss):.4f}\")\n",
    "\n",
    "    \n",
    "    # Explicitly verify we're using GPU for at least one operation\n",
    "    \"\"\"with tf.device('/GPU:0'):\n",
    "        test_tensor = tf.random.normal([1000, 1000])\n",
    "        test_result = tf.matmul(test_tensor, test_tensor)\n",
    "        # If this executes without error, GPU is working\n",
    "        print(f\"GPU Test - Tensor shape: {test_result.shape}\")\n",
    "        \n",
    "    print(\"Memory before training:\")\n",
    "    monitor_gpu_memory()\n",
    "    # Then modify these training parameters\n",
    "    model.train_with_checkpoint(\n",
    "        df=df,\n",
    "        batch_size=32,               # REDUCED from 256 to 32\n",
    "        max_epochs=50,\n",
    "        val_split=0.05,\n",
    "        resume_from=None,\n",
    "        save_every=1\n",
    "    )\"\"\"\n",
    "    \n",
    "    # Generate molecules using the trained model\n",
    "    target_protein = df['Target'].iloc[0]  # Use first target as example\n",
    "    desired_affinity = 0.8  # nM, adjust as needed\n",
    "    \n",
    "    generated_molecules = model.generate_molecules(\n",
    "        target_protein,\n",
    "        desired_affinity,\n",
    "        n_samples=20\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3411237,
     "sourceId": 5945100,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
